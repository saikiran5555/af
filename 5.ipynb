{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "584fec89",
   "metadata": {},
   "source": [
    "The Rectified Linear Unit (ReLU) activation function is a piecewise linear function that returns 0 for negative input values and returns the input value itself for positive input values. Mathematically, it can be defined as:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "max\n",
    "⁡\n",
    "(\n",
    "0\n",
    ",\n",
    "�\n",
    ")\n",
    "f(x)=max(0,x)\n",
    "In simpler terms, if the input \n",
    "�\n",
    "x is negative, ReLU outputs 0; otherwise, it outputs the input value unchanged.\n",
    "\n",
    "Here's how ReLU differs from the sigmoid function:\n",
    "\n",
    "ReLU Activation Function:\n",
    "Non-saturating: ReLU does not saturate in the positive region, meaning it does not approach an asymptote for large positive input values. This helps mitigate the vanishing gradient problem and accelerates convergence in deep neural networks.\n",
    "Sparse Activation: ReLU can output zero for negative inputs, effectively turning off neurons that are not activated. This can introduce sparsity into the network, which may be beneficial for regularization and computational efficiency.\n",
    "Simple Computation: ReLU has a simple mathematical formulation and is computationally efficient to compute compared to more complex functions like sigmoid or tanh.\n",
    "Sigmoid Activation Function:\n",
    "Saturating: The sigmoid function saturates at 0 and 1 for very negative and very positive inputs, respectively. This means that for large positive or negative inputs, the gradient becomes very small, leading to the vanishing gradient problem.\n",
    "Smooth Transition: Sigmoid provides a smooth, continuous output between 0 and 1, which can be interpreted as a probability in binary classification tasks. However, this smoothness also contributes to the vanishing gradient problem, especially in deep networks.\n",
    "Computationally Expensive: The sigmoid function involves exponential operations, which can be computationally expensive, especially when dealing with large datasets or deep networks.\n",
    "In summary, while both ReLU and sigmoid are activation functions used in neural networks, ReLU is preferred in many cases due to its non-saturating nature, sparsity-inducing properties, and computational efficiency, especially in deep learning architectures. However, the choice of activation function depends on the specific requirements of the task and the characteristics of the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
