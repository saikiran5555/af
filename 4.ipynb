{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c3a17a4",
   "metadata": {},
   "source": [
    "The sigmoid activation function, also known as the logistic function, is a non-linear function commonly used in neural networks. It maps any real-valued number to a value between 0 and 1. The sigmoid function is defined as:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "1\n",
    "1\n",
    "+\n",
    "�\n",
    "−\n",
    "�\n",
    "σ(x)= \n",
    "1+e \n",
    "−x\n",
    " \n",
    "1\n",
    "​\n",
    " \n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "Range: The output of the sigmoid function is always in the range \n",
    "(\n",
    "0\n",
    ",\n",
    "1\n",
    ")\n",
    "(0,1). This makes it useful for binary classification tasks, where the output can be interpreted as a probability.\n",
    "\n",
    "Smoothness: The sigmoid function is smooth and continuously differentiable, which is advantageous for gradient-based optimization algorithms used in training neural networks, such as gradient descent.\n",
    "\n",
    "Non-linearity: Sigmoid introduces non-linearity into the network, enabling it to learn complex relationships in the data.\n",
    "\n",
    "However, the sigmoid function has several disadvantages:\n",
    "\n",
    "Vanishing Gradients: For very large positive or negative inputs, the gradient of the sigmoid function becomes close to zero. This can cause the gradients to vanish during backpropagation, leading to slow convergence or the so-called \"vanishing gradient\" problem. This issue can make training deep neural networks difficult.\n",
    "\n",
    "Output Saturation: Sigmoid outputs values close to 0 or 1 when the input is very negative or positive, respectively. This leads to saturation of the neuron, where small changes in the input result in little to no change in the output. This can slow down learning, especially in the presence of vanishing gradients.\n",
    "\n",
    "Not Centered at Zero: The output of the sigmoid function is not centered at zero, which can lead to issues in weight initialization and optimization, especially in deeper networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
