{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b7b30e4",
   "metadata": {},
   "source": [
    "Activation functions are critical components of neural networks as they introduce non-linearity, allowing neural networks to learn complex patterns in data. Here are some common types of activation functions used in neural networks:\n",
    "\n",
    "Sigmoid Function (Logistic):\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "1\n",
    "1\n",
    "+\n",
    "�\n",
    "−\n",
    "�\n",
    "σ(x)= \n",
    "1+e \n",
    "−x\n",
    " \n",
    "1\n",
    "​\n",
    " \n",
    "\n",
    "Outputs values between 0 and 1.\n",
    "Historically used in older models but has fallen out of favor due to issues like vanishing gradients.\n",
    "Hyperbolic Tangent (Tanh):\n",
    "tanh\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "�\n",
    "�\n",
    "−\n",
    "�\n",
    "−\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "−\n",
    "�\n",
    "tanh(x)= \n",
    "e \n",
    "x\n",
    " +e \n",
    "−x\n",
    " \n",
    "e \n",
    "x\n",
    " −e \n",
    "−x\n",
    " \n",
    "​\n",
    " \n",
    "\n",
    "Outputs values between -1 and 1.\n",
    "Similar to sigmoid but centered at 0, which helps with optimization.\n",
    "Rectified Linear Unit (ReLU):\n",
    "ReLU\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "max\n",
    "⁡\n",
    "(\n",
    "0\n",
    ",\n",
    "�\n",
    ")\n",
    "ReLU(x)=max(0,x)\n",
    "\n",
    "Outputs the input itself if positive, otherwise outputs zero.\n",
    "Provides sparsity and alleviates the vanishing gradient problem.\n",
    "Leaky ReLU:\n",
    "LeakyReLU\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "{\n",
    "�\n",
    "if \n",
    "�\n",
    ">\n",
    "0\n",
    "�\n",
    "�\n",
    "otherwise\n",
    "LeakyReLU(x)={ \n",
    "x\n",
    "αx\n",
    "​\n",
    "  \n",
    "if x>0\n",
    "otherwise\n",
    "​\n",
    " \n",
    "\n",
    "Introduces a small slope (typically 0.01) for negative values, helping to address the \"dying ReLU\" problem.\n",
    "Parametric ReLU (PReLU):\n",
    "PReLU\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "{\n",
    "�\n",
    "if \n",
    "�\n",
    ">\n",
    "0\n",
    "�\n",
    "�\n",
    "otherwise\n",
    "PReLU(x)={ \n",
    "x\n",
    "αx\n",
    "​\n",
    "  \n",
    "if x>0\n",
    "otherwise\n",
    "​\n",
    " \n",
    "\n",
    "Similar to Leaky ReLU but with the slope parameter \n",
    "�\n",
    "α being learned during training.\n",
    "Exponential Linear Unit (ELU):\n",
    "ELU\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "{\n",
    "�\n",
    "if \n",
    "�\n",
    ">\n",
    "0\n",
    "�\n",
    "(\n",
    "�\n",
    "�\n",
    "−\n",
    "1\n",
    ")\n",
    "otherwise\n",
    "ELU(x)={ \n",
    "x\n",
    "α(e \n",
    "x\n",
    " −1)\n",
    "​\n",
    "  \n",
    "if x>0\n",
    "otherwise\n",
    "​\n",
    " \n",
    "\n",
    "Smooth approximation of ReLU with some benefits including robustness to noise.\n",
    "Scaled Exponential Linear Unit (SELU):\n",
    "\n",
    "Similar to ELU but with a different scaling factor.\n",
    "Designed to be self-normalizing, which can lead to better convergence properties.\n",
    "These activation functions serve different purposes and may be used depending on the specific requirements of the neural network architecture and the nature of the problem being solved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
